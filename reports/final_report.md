# CSDS 395 - Final Report: An All-In-One NLP Stock Market Backtester

> Shaochen (Henry) ZHONG `sxz517`
> Jiaqi Yu `jxy618`
> Mocun Ye `mxy293`

---
*Due and submitted on 11/30/2020, schedule to present on 12/14/2020*

---


## Background

The background of the project remains largely unchanged from our perviously submitted proposal and report. So here we will provide a concentrated version of it.

Using NLP related approaches to do some kinds of prediction on stock market is nothing new among traders who want to develop profitable trading strategies, researchers who want to testify their models' performances, and also to every developer who wants to have some hand-on ML/DL experience.

Despite the popularity, we noticed that it is rather hard to verify a NLP-stock-prediction model's performance since the researcher will have to gather the plain text data, gather the company data, gather the stock market data, and categorize them in a way that is communicable with each other and the model; then the researcher will need to build a virtual trading platform that keeps track of all the trading signals generated by the model, and visualize them for evaluation.

To implement all these steps from ground up, it is required for a researcher to have certain level of proficiency on skills which are, from a research stand-point, fairly deviated from the nature of the NLP model itself (like scraping a website and understanding the fundamental mechanism of trading in stock market). Even though there are some very mature tools being developed in the subareas of this task (especially on the stock market backtesting area), it still requires a reasonably large amount of effort to couple them together, and to store necessary information in a way that are not only communicable with each other, but also suits to the design of each and every tool a research chose to use. It is our understanding this kind of preliminary work will distract a researcher from the essence of his/her work -- developing an SOTA model, and will also create a unnecessary barriers for researcher who wants to quickly testify an idea in a controlled manner, or to who are in the need of reproducing a published work.


We like to build a set of tools that may automate such process to a certain degree. The ideal workflow we visioned is that developers may import their plain text and company data in a certain format (or even use the build-in API to obtain such data, of course, with limitation on available channels), then we will have a set of functions (or parsers) available to execute and register the trading signals generated by a desired model; our toolkit will also able research to restructure data in a way that is suitable for his/her model. In our pervious proposal we said if time’s available, we may even built in some classic NLP model just to provide a benchmark reference on “the same playing field,” or develop my own HMM model I am researching right now for demo. **In our [pervious report](https://github.com/choH/ALO_NLP_backtester/blob/master/reports/progress_report_1.md), by re-evaluating of our team members' course load per TA's feedback, we have dropped the idea of actually providing a technically significant model, but just to place a "dummy model" to demonstrate our project's capability.**
**(update since [Progress Report 2](https://github.com/choH/ALO_NLP_backtester/blob/master/reports/progress_report_2.md), we indeed end up making a dummy model)**

In our pervious proposal, we also researched couple related works regarding this task (mostly focusing on the fields of backtesting, obtaining stock market data, and obtaining plain text data). We analyzed the pros-and-cons of several existing public available libraries and described why are (or why aren't) we developing own our tools, and what functionalities were expected. Please do review our [*Proposal*](https://github.com/choH/ALO_NLP_backtester/blob/master/reports/proposal.md) if you'd expect more information regarding related works.

