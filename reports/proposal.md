# CSDS 395 - Senior Project Proposal: An All-In-One NLP Stock Market Backtester

> Shaochen (Henry) ZHONG `sxz517`
> Jiaqi Yu `jxy618`
> Mocun Ye `mxy293`

---

## Technical Project Requirements and Specifications

Using NLP related approaches to do some kinds of prediction on stock market is nothing new among traders who want to develop profitable trading strategies, researchers who want to testify their models' performances, and also to every developer who wants to have some hand-on ML/DL experience. In our understanding, the extreme popularity of this area can be attribute to:

1. NLP is an intrinsically fascinating subject in between human culture and computers, and since language is what we human use to communicate with each other, the channel of data input can be virtually unlimited.
2. It provides a benchmark that is, in a sense, closer to "real-word application" many other dataset benchmarks.
3. The success of a model will lead to direct financial gain. Also a performance of a model can be easily quantified.

Despite the popularity, we noticed that it is rather hard to verify a NLP-stock-prediction model since the researcher will have to gather the plain text data, gather the company data, gather the stock market data, and categorize them in a way that is communicable with each other and the model; then the researcher will need to build a virtual trading platform that keeps track of all the trading signals generated by the model, and visualize them for evaluation.

To implement all these steps from ground up, it is required for a researcher to have certain level of proficiency on skills which are, from a research stand-point, fairly deviated from the nature of the NLP model itself (like scraping a website and understanding the fundamental mechanism of trading in stock market). Even though there are some very mature tools being developed in the subareas of this task (especially on the stock market backtesting area), it still requires a reasonably large amount of effort to coupling them together, and to store necessary information in a way that are not only communicable with each other, but also suits to the design of each and every tool a research chose to use. It is our understanding this kind of preliminary work will distract a researcher from the essence of his/her work -- developing an SOTA model, and also create a unnecessary barrier for researcher who wants to quickly testify an idea in a controlled manner, or to who are in the need of reproducing a published work.

## Proposed Solutions

We like to build a set of tool that may automate such process for a certain degree. The ideal workflow we visioned is that developers may import their plain text and company data in a certain format (or even use the build-in API to obtain such data, of course, with limitation on available channels), then we will have a set of functions (or parsers) available to execute and register the trading signals generated by a desired model; our toolkit will also able research to restructure data in a way that is suitable for his/her model. If time’s available, we may even built in some classic NLP model just to provide a benchmark reference on “the same playing field,” or develop my own HMM model I am researching right now for demo.

## Background / Related Work

The concept of quantum trading wasn't widespread until approximately four years ago. In the early days, developers not only design their own trading models, but also have to collect historical trading data, do the data formatting and regularization, and write testing platforms all by themselves, which used to be a tremendous amount of work. Since then, an increasing amount of people have started working on quantum trading, and a lot of related tools and data sources have emerged. Here we list a few major platforms that are either data sources or backtesting toolkits. We would analysis them on their functionalities, ease of use, and the aspects that they miss and that our platform would provide.

### Data Providers

#### Seeking Alpha

Seeking Alpha is a sophisticated stock data service that has been living since 2004. They are now providing a varies of stock-related data services, including the trading history and many other useful information, such as news and company announcements. However, despite its being feature rich, it doesn't directly provide a data-fetching API that could easily used by the programmer. One could only either write their own bot to parse their website, or use some third-party API that are mostly only capable of extracting only parts of the information from Seeking Alpha.

Our solution is expected to offer an easy way for the user to access and manage stock data. Our implementation would include tools that extract data from popular data platforms and formatting them to be compatible with our backtesting platform without losing any features from the original source.

#### Finnhub.io

Finnhub.io is a newly born data service that aims to provide trading data specialized for quantum trading. Their site offers a comprehensive set of APIs that provide easy access to a wide variety of data that one, or one's model, might need for analysis. This platform serves as a good data services, and we consider it being one of the references for building the data structure of our platform.

### Backtesting Platform

#### Python-Backtesting

Python has included a backtesting library, Backtesting.py, in its official repository since approximately two years ago. The package was still in beta testing, and is at version 0.2.2 as of the time writing this proposal. Backtesting.py offers a simple yet powerful trading simulator, which accepts data and a user-written trading bot, and produces the trading result. The platform provides efficient computing modules and a list of professional statistics in its result.

However, the downside of this module is that it offers no data manipulation and management tools. The module only accepts only simplest datasets that contains a list of time and prices, which is very insufficient for simulating real world situations.

Our integrated solution, in this respect, is planned to provide built-in data manipulation tools. This includes not only the methods that extracts data from data sources, slicing and storing the data, but also supports for varies datasets features. For example, we would support scalable timeframe from minutes to years and discontinuity in the timeline. We would also support events like date changing, splitting, merging, etc. Those events could be optionally adopted by the models, so that the complexity of the models is also scalable.


### Plain Text Data Provider

For scraper, taking scraping WSJ as an example, most scrapers like [`wsj_scraper`](https://github.com/daomingyin/wsj_scraper) have the correct idea, but cannot be used due to:

1. Lack of maintenance, so they are not usable upon the updated structure of the news distributor.
2. Lack of robustness, as many of them are written in a "one-off" fashion with little to no system design thinking implemented.
3. Lack of metadata scraping on the fly. As many of then only store the "plain text new articles" and that's it. However, metadata of the an article, like its mentioned company, its date, its category... are often extremely valueable.

We will mostly working on regulating the role of a scraper and how will it communicate with other modules of the project, so that a community — if we will ever have one — will be able to contribute upon the design and keeping everything up-to-date. We will also provide a scraper on WSJ for demo.


## Work to be Done

This project, due to its "all-in-one" nature, requires development on a lot of area. The five main modules I can think of are:

1. A scraper which can scrape NLP time-series data from a channel, and store them in a certain regulated manner. It will at least include both the original plain-text data (e.g. news articles) and meta-data (e.g. the ticker, exchange, market information of a company we intend to trade on).
2. A virtual trading platform which capable of obtaining stock market data within a certain timeframe, register trading signals, and reflects on its "virtual balance." It will also be able generate an informative trade log upon each episode.
3. A data organizer that can take in data from different channels, and normalize them together in a way that is easy to read / input to model.
4. A visualizer that can demonstrate the trading signals of a model, and evaluate it accordingly.
5. A demo model.

Other than the pure "coding" portion of the development, due to the wide coverage of the project, a thoughtful system deign and management on Git collaboration must be done. We will also provide documentation, manual, and demo work when necessary.

## Management Plan

We set up a rough bi-weekly plan through out the semester to monitor our progress. Note this plan is subject to change, especially after the system design is done.

* Week of 09/07/2020: System design
* Week of 09/14/2020: System design and task assignment
* Week of 09/21/2020: Regulate input/output format between each module
* Week of 09/28/2020: Refactor scraper / Technical selection on backtester / Learning for visualizer / Learning stock market basic
* Week of 10/05/2020: Refactor scraper / Virtual trading platform development / Regulate trade log format / Visualizer development
* Week of 10/12/2020: Virtual trading platform development / Regulate trade log format / Visualizer development / Data organizer development
* Week of 10/19/2020: Test coupling between virtual trading platform + scraper / Visualizer development
* Week of 10/26/2020: Test coupling everything.
* Week of 11/02/2020: Dummy demo model development
* Week of 11/09/2020: Debug / Report writing
* Week of 11/16/2020: Redundancy
* Week of 11/23/2020: Redundancy
